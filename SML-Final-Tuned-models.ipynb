{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded ...\n",
      "(1013, 13)\n",
      "Training and Testing data are Scaled ..\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# import data\n",
    "dataset = pd.read_csv('D:\\\\cleanedData.csv')\n",
    "print(\"Dataset Loaded ...\")\n",
    "print(dataset.shape)\n",
    "\n",
    "\n",
    "#Split dataset into training and test \n",
    "train, test = train_test_split(dataset, test_size=0.2, shuffle=True)\n",
    "\n",
    "xtrain = train.iloc[:, 0:-1].values\n",
    "ytrain = train.iloc[:, -1].values\n",
    "xtest  = test.iloc[:, 0:-1].values\n",
    "ytest  = test.iloc[:, -1].values\n",
    "\n",
    "##Data Scalling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "xtrain = sc.fit_transform(xtrain)\n",
    "xtest  = sc.transform(xtest)\n",
    "\n",
    "print(\"Training and Testing data are Scaled ..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "[[153   6]\n",
      " [ 14  30]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       159\n",
      "           1       0.83      0.68      0.75        44\n",
      "\n",
      "    accuracy                           0.90       203\n",
      "   macro avg       0.87      0.82      0.84       203\n",
      "weighted avg       0.90      0.90      0.90       203\n",
      "\n",
      "------------------------------------\n",
      "Weighted Logistic Regression:\n",
      "[[130  29]\n",
      " [  5  39]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.82      0.88       159\n",
      "           1       0.57      0.89      0.70        44\n",
      "\n",
      "    accuracy                           0.83       203\n",
      "   macro avg       0.77      0.85      0.79       203\n",
      "weighted avg       0.88      0.83      0.84       203\n",
      "\n",
      "------------------------------------\n",
      "best Logistic Regression:\n",
      "[[151   8]\n",
      " [ 13  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       159\n",
      "           1       0.79      0.70      0.75        44\n",
      "\n",
      "    accuracy                           0.90       203\n",
      "   macro avg       0.86      0.83      0.84       203\n",
      "weighted avg       0.89      0.90      0.89       203\n",
      "\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lina\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "## Logistic Regression \n",
    "LogReg = LogisticRegression()\n",
    "LogReg.fit(xtrain, ytrain)\n",
    "y_pred=LogReg.predict(xtest)\n",
    "print('Logistic Regression:')\n",
    "print(confusion_matrix(ytest, y_pred))\n",
    "print(classification_report(ytest, y_pred))\n",
    "# [[223  10]\n",
    "# [ 33  46]]\n",
    "print('------------------------------------')\n",
    "\n",
    "## Weighted logistic Regression \n",
    "w = {0:0.20, 1:0.80}\n",
    "#lg2 = LogisticRegression(random_state=13, class_weight=w)\n",
    "lg2 = LogisticRegression(class_weight=w)\n",
    "lg2.fit(xtrain, ytrain)\n",
    "y_pred = lg2.predict(xtest)\n",
    "\n",
    "print('Weighted Logistic Regression:')\n",
    "print(confusion_matrix(ytest, y_pred))\n",
    "print(classification_report(ytest, y_pred))\n",
    "print('------------------------------------')\n",
    "\n",
    "\n",
    "lg3 = LogisticRegression( C=3.787878787878788, multi_class= 'ovr', penalty= 'l1', random_state= 3, solver= 'liblinear')\n",
    "lg3.fit(xtrain, ytrain)\n",
    "y_pred = lg3.predict(xtest)\n",
    "\n",
    "print('best Logistic Regression:')\n",
    "print(confusion_matrix(ytest, y_pred))\n",
    "print(classification_report(ytest, y_pred))\n",
    "\n",
    "print('------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Decision Tree: \n",
      "[[144   9]\n",
      " [ 27  23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89       153\n",
      "           1       0.72      0.46      0.56        50\n",
      "\n",
      "    accuracy                           0.82       203\n",
      "   macro avg       0.78      0.70      0.72       203\n",
      "weighted avg       0.81      0.82      0.81       203\n",
      "\n",
      "------------------------------------\n",
      "Entropy Decision Tree: \n",
      "[[141  12]\n",
      " [ 25  25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88       153\n",
      "           1       0.68      0.50      0.57        50\n",
      "\n",
      "    accuracy                           0.82       203\n",
      "   macro avg       0.76      0.71      0.73       203\n",
      "weighted avg       0.81      0.82      0.81       203\n",
      "\n",
      "------------------------------------\n",
      "Random Forest: \n",
      "[[148   5]\n",
      " [ 28  22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90       153\n",
      "           1       0.81      0.44      0.57        50\n",
      "\n",
      "    accuracy                           0.84       203\n",
      "   macro avg       0.83      0.70      0.74       203\n",
      "weighted avg       0.83      0.84      0.82       203\n",
      "\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Decision trees and Random Forests\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "      \n",
    "## Create gini Decision tree\n",
    "DecTree1 = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=10, min_samples_leaf=8)\n",
    "DecTree1.fit(xtrain, ytrain)\n",
    "y_pred1 = DecTree1.predict(xtest)\n",
    "print(\"Gini Decision Tree: \")\n",
    "print(confusion_matrix(ytest, y_pred1))\n",
    "print(classification_report(ytest, y_pred1))\n",
    "print('------------------------------------')\n",
    "\n",
    "        \n",
    "## Decision tree with entropy\n",
    "\n",
    "## Best DecTree parameters \n",
    "##DecTree2 = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth = 10, min_samples_leaf = 8)\n",
    "## [[217  16]\n",
    "## [ 31  48]]\n",
    "DecTree2 = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth = 8, min_samples_leaf = 8)\n",
    "DecTree2.fit(xtrain, ytrain)\n",
    "y_pred2 = DecTree2.predict(xtest)\n",
    "print(\"Entropy Decision Tree: \")\n",
    "print(confusion_matrix(ytest, y_pred2))\n",
    "print(classification_report(ytest, y_pred2))\n",
    "print('------------------------------------')\n",
    "\n",
    "\n",
    "## Random Forest classifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(criterion='entropy', n_estimators=30, max_depth=7, max_features='log2', random_state=3, n_jobs=2)\n",
    "#forest = RandomForestClassifier(criterion='gini', n_estimators=20, random_state=4, n_jobs=8)\n",
    "#[[227  12]\n",
    "# [ 42  31]]\n",
    "forest.fit(xtrain, ytrain)\n",
    "y_pred3 = forest.predict(xtest)\n",
    "print(\"Random Forest: \")\n",
    "print(confusion_matrix(ytest, y_pred3))\n",
    "print(classification_report(ytest, y_pred3))\n",
    "print('------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminative Analysis: \n",
      "[[147   6]\n",
      " [ 21  29]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       153\n",
      "           1       0.83      0.58      0.68        50\n",
      "\n",
      "    accuracy                           0.87       203\n",
      "   macro avg       0.85      0.77      0.80       203\n",
      "weighted avg       0.86      0.87      0.86       203\n",
      "\n",
      "------------------------------------\n",
      "Mean Accuracy: 0.893\n",
      "Config: {'solver': 'svd'}\n",
      "Mean Accuracy: 0.893\n",
      "Config: {'shrinkage': 0.0}\n"
     ]
    }
   ],
   "source": [
    "## Discriminative Analysis \n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "DiscModel = LinearDiscriminantAnalysis(solver='lsqr', shrinkage= 0.0)\n",
    "DiscModel.fit(xtrain, ytrain)\n",
    "y_pred = DiscModel.predict(xtest)\n",
    "print(\"Discriminative Analysis: \")\n",
    "print(confusion_matrix(ytest, y_pred))\n",
    "print(classification_report(ytest, y_pred))\n",
    "print('------------------------------------')\n",
    "\n",
    "#[[225  14]\n",
    "# [ 36  37]]\n",
    "\n",
    "\n",
    "## The following is for tuning the model more \n",
    "\n",
    "# grid search solver for lda\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n",
    "# define model\n",
    "model = LinearDiscriminantAnalysis()\n",
    "# define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['solver'] = ['svd', 'lsqr', 'eigen']\n",
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X, y)\n",
    "# summarize\n",
    "print('Mean Accuracy: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)\n",
    "\n",
    "\n",
    "# grid search shrinkage for lda\n",
    "from numpy import arange\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n",
    "# define model\n",
    "model = LinearDiscriminantAnalysis(solver='lsqr')\n",
    "# define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define grid\n",
    "grid = dict()\n",
    "grid['shrinkage'] = arange(0, 1, 0.01)\n",
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X, y)\n",
    "# summarize\n",
    "print('Mean Accuracy: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-nearest neighbour  \n",
      "[[141  12]\n",
      " [ 25  25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.89       153\n",
      "           1       0.79      0.38      0.51        50\n",
      "\n",
      "    accuracy                           0.82       203\n",
      "   macro avg       0.81      0.67      0.70       203\n",
      "weighted avg       0.82      0.82      0.80       203\n",
      "\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## KNN\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=8, p=1, weights='distance', algorithm='auto')\n",
    "knn.fit(xtrain, ytrain)\n",
    "y_pred = knn.predict(xtest)\n",
    "print(\"K-nearest neighbour  \")\n",
    "print(confusion_matrix(ytest, y_pred2))\n",
    "print(classification_report(ytest, y_pred))\n",
    "print('------------------------------------')\n",
    "\n",
    "\n",
    "## For later ------------------------------------------------------------------------------------------\n",
    "# Create a pipeline\n",
    "#pipeline = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "#param_grid = [{\n",
    "#    'kneighborsclassifier__n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "#    'kneighborsclassifier__p': [1, 2],\n",
    "#    'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "#    'kneighborsclassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "#}]\n",
    "\n",
    "# Create a grid search instance\n",
    "#gs = GridSearchCV(pipeline, param_grid = param_grid,\n",
    "#                  scoring='accuracy',\n",
    "#                  refit=True,\n",
    "#                  cv=10,\n",
    "#                  verbose=1,\n",
    "#                  n_jobs=2)\n",
    "#\n",
    "# Fit the most optimal model\n",
    "#gs.fit(X_train, y_train)\n",
    "# Print the best model parameters and scores\n",
    "#print('Best Score: %.3f' % gs.best_score_, '\\nBest Parameters: ', gs.best_params_)\n",
    "#print('Score: %.3f' % gs.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_356/936816648.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Neural Networks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "## Neural Networks\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "NNmodel = Sequential()\n",
    "#saved as model 6 - best one so far \n",
    "#NNmodel.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13)) \n",
    "#NNmodel.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))           \n",
    "\n",
    "NNmodel.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu', input_dim = 16)) \n",
    "NNmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))           \n",
    "#NNmodel.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))           \n",
    "\n",
    "\n",
    "NNmodel.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.01)\n",
    "NNmodel.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "print(\"NN created ..\")\n",
    "\n",
    "NNmodel.fit(xtrain, ytrain, batch_size = 10, epochs = 70)\n",
    "print(\"End of training ...\")\n",
    "\n",
    "y_NNresults = NNmodel.predict(xtest)\n",
    "y_NNresults = (y_NNresults > 0.5)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "print(confusion_matrix(ytest, y_NNresults))\n",
    "print(classification_report(ytest, y_NNresults))\n",
    "print(\"End of testing ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "os.chdir('D:\\\\SML\\\\')\n",
    "\n",
    "         \n",
    "# serialize model to JSON\n",
    "model_json = NNmodel.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "NNmodel.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
